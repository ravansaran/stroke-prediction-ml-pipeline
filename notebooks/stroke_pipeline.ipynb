{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa78895",
   "metadata": {},
   "source": [
    "# Stroke Prediction â€” Clean ML Pipeline\n",
    "This notebook builds a reproducible **scikit-learn** pipeline for a stroke dataset.\n",
    "It includes preprocessing, class imbalance handling, model comparison, ROC/PR curves, and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9926ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, roc_curve, precision_recall_curve,\n",
    "                             confusion_matrix, classification_report)\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Config ---\n",
    "use_demo = True   # set to False to use data/stroke.csv\n",
    "data_dir = Path('../data')\n",
    "demo_path = data_dir/'sample.csv'\n",
    "real_path = data_dir/'stroke.csv'  # provide your own\n",
    "\n",
    "# Load\n",
    "path = demo_path if use_demo else real_path\n",
    "df = pd.read_csv(path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac173c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Basic EDA-like checks\n",
    "print('Shape:', df.shape)\n",
    "print('Columns:', list(df.columns))\n",
    "print(df.isna().sum().sort_values(ascending=False).head(10))\n",
    "print(df['stroke'].value_counts(normalize=True).rename('class balance'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833aeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split\n",
    "X = df.drop(columns=['stroke'])\n",
    "y = df['stroke'].astype(int)\n",
    "\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "numeric_pre = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale', StandardScaler())\n",
    "])\n",
    "categorical_pre = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', numeric_pre, num_cols),\n",
    "    ('cat', categorical_pre, cat_cols)\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25,\n",
    "                                                    random_state=42, stratify=y)\n",
    "\n",
    "print('Train size:', X_train.shape, ' Test size:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62ceb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Models\n",
    "logreg = Pipeline([('prep', preprocess),\n",
    "                   ('clf', LogisticRegression(max_iter=200, class_weight='balanced'))])\n",
    "\n",
    "rf = Pipeline([('prep', preprocess),\n",
    "               ('clf', RandomForestClassifier(n_estimators=400, random_state=42, class_weight='balanced'))])\n",
    "\n",
    "models = {'LogReg': logreg, 'RandomForest': rf}\n",
    "\n",
    "# Fit & evaluate\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:,1]\n",
    "    preds = (proba >= 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    ap = average_precision_score(y_test, proba)\n",
    "    results[name] = {'auc': auc, 'ap': ap, 'preds': preds, 'proba': proba, 'model': model}\n",
    "    print(f\"{name}: ROC-AUC={auc:.3f} | PR-AUC={ap:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2141efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ROC and PR curves\n",
    "plt.figure(figsize=(6,4))\n",
    "for name, r in results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, r['proba'])\n",
    "    plt.plot(fpr, tpr, label=f\"{name} (AUC={r['auc']:.3f})\")\n",
    "plt.plot([0,1],[0,1],'--', lw=1)\n",
    "plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curves'); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "for name, r in results.items():\n",
    "    prec, rec, _ = precision_recall_curve(y_test, r['proba'])\n",
    "    plt.plot(rec, prec, label=f\"{name} (AP={r['ap']:.3f})\")\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title('Precision-Recall Curves'); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9595ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrix and report for the best model by PR-AUC (class imbalance aware)\n",
    "best = max(results.items(), key=lambda kv: kv[1]['ap'])[1]\n",
    "best_name = max(results.items(), key=lambda kv: kv[1]['ap'])[0]\n",
    "print('Best by PR-AUC:', best_name)\n",
    "print(confusion_matrix(y_test, best['preds']))\n",
    "print(classification_report(y_test, best['preds']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ba6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Permutation importance (on a held-out set) for the best model\n",
    "best_model = best['model']\n",
    "# Pull the random forest from the pipeline for importances (logreg won't have .feature_importances_)\n",
    "if best_name == 'RandomForest':\n",
    "    # get feature names after preprocessing\n",
    "    ohe = best_model.named_steps['prep'].named_transformers_['cat'].named_steps['onehot']\n",
    "    num_cols = best_model.named_steps['prep'].transformers_[0][2]\n",
    "    cat_cols = ohe.get_feature_names_out(best_model.named_steps['prep'].transformers_[1][2])\n",
    "    feat_names = np.concatenate([num_cols, cat_cols])\n",
    "\n",
    "    # permutation importance\n",
    "    r = permutation_importance(best_model, X_test, y_test, n_repeats=5, random_state=42)\n",
    "    importances = r.importances_mean\n",
    "    order = np.argsort(importances)[::-1][:15]\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.barh(range(len(order)), importances[order][::-1])\n",
    "    plt.yticks(range(len(order)), [feat_names[i] for i in order][::-1])\n",
    "    plt.xlabel('Mean decrease in score'); plt.title('Permutation Importance (top 15)'); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print('Permutation importance shown for RandomForest only (tree-based).')\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
